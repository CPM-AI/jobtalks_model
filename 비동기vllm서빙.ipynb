{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91353880-01ba-41c2-96f9-4b397db7f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import AsyncLLMEngine, SamplingParams\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "import json\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import StreamingResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f4d537-0850-4251-8679-3cdded23e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_args = AsyncEngineArgs(model=\"cpm-ai/Ocelot-Ko-self-instruction-10.8B-v1.0\", \n",
    "                              tensor_parallel_size=8, \n",
    "                              max_model_len=4096, \n",
    "                              gpu_memory_utilization=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48f5f6a-86d9-4b6b-a7cc-06eecef0a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-06-13 08:24:49,629\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 08:24:50 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='cpm-ai/Ocelot-Ko-self-instruction-10.8B-v1.0', speculative_config=None, tokenizer='cpm-ai/Ocelot-Ko-self-instruction-10.8B-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=cpm-ai/Ocelot-Ko-self-instruction-10.8B-v1.0)\n",
      "INFO 06-13 08:25:17 utils.py:618] Found nccl from library libnccl.so.2\n",
      "INFO 06-13 08:25:17 pynccl.py:65] vLLM is using nccl==2.20.5\n",
      "\u001b[36m(RayWorkerWrapper pid=2689428)\u001b[0m INFO 06-13 08:25:17 utils.py:618] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(RayWorkerWrapper pid=2689428)\u001b[0m INFO 06-13 08:25:17 pynccl.py:65] vLLM is using nccl==2.20.5\n",
      "INFO 06-13 08:25:19 custom_all_reduce_utils.py:179] reading GPU P2P access cache from /home/ubuntu/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[36m(RayWorkerWrapper pid=2689428)\u001b[0m INFO 06-13 08:25:19 custom_all_reduce_utils.py:179] reading GPU P2P access cache from /home/ubuntu/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 06-13 08:25:20 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerWrapper pid=2689428)\u001b[0m INFO 06-13 08:25:20 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-13 08:25:23 model_runner.py:146] Loading model weights took 2.5173 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=2690086)\u001b[0m INFO 06-13 08:25:23 model_runner.py:146] Loading model weights took 2.5173 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=2690086)\u001b[0m INFO 06-13 08:25:17 utils.py:618] Found nccl from library libnccl.so.2\u001b[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2690086)\u001b[0m INFO 06-13 08:25:17 pynccl.py:65] vLLM is using nccl==2.20.5\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2690086)\u001b[0m INFO 06-13 08:25:19 custom_all_reduce_utils.py:179] reading GPU P2P access cache from /home/ubuntu/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2689616)\u001b[0m INFO 06-13 08:25:21 weight_utils.py:207] Using model weights format ['*.safetensors']\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "INFO 06-13 08:25:30 distributed_gpu_executor.py:56] # GPU blocks: 3570, # CPU blocks: 10922\n",
      "\u001b[36m(RayWorkerWrapper pid=2690086)\u001b[0m INFO 06-13 08:25:34 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=2690086)\u001b[0m INFO 06-13 08:25:34 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=2689896)\u001b[0m INFO 06-13 08:25:25 model_runner.py:146] Loading model weights took 2.5173 GB\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "INFO 06-13 08:25:34 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-13 08:25:34 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-13 08:25:41 custom_all_reduce.py:260] Registering 3395 cuda graph addresses\n",
      "\u001b[36m(RayWorkerWrapper pid=2689428)\u001b[0m INFO 06-13 08:25:41 custom_all_reduce.py:260] Registering 3395 cuda graph addressesINFO 06-13 08:25:41 model_runner.py:924] Graph capturing finished in 7 secs.\n",
      "\n",
      "\u001b[36m(RayWorkerWrapper pid=2689991)\u001b[0m INFO 06-13 08:25:35 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2689991)\u001b[0m INFO 06-13 08:25:35 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=2689428)\u001b[0m INFO 06-13 08:25:41 model_runner.py:924] Graph capturing finished in 6 secs.\n",
      "\u001b[36m(RayWorkerWrapper pid=2689616)\u001b[0m INFO 06-13 08:25:41 model_runner.py:924] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "engine = AsyncLLMEngine.from_engine_args(engine_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af28800c-cda6-4063-9d4c-25aa4d2d85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0, top_p=0.95, top_k=50, max_tokens=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "623d1ac7-1e8b-4157-8239-e8801b6272aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params_tl = SamplingParams(temperature=0.9, top_p=0.90, top_k=5, max_tokens=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9930bd89-df4b-4539-9ed3-5917d250eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\", \"https://www.cpm-ai.site\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class InputText(BaseModel):\n",
    "    text: str\n",
    "    item: str\n",
    "\n",
    "class TalentText(BaseModel):\n",
    "    req_id: str\n",
    "    talent: str\n",
    "    text: str\n",
    "\n",
    "# 프론트단 실시간 추론\n",
    "queue = asyncio.Queue()\n",
    "\n",
    "@app.get(\"/sse_01\", response_class=StreamingResponse)\n",
    "async def sse(request: Request):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"text/event-stream;charset=utf-8\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "    async def event_generator():\n",
    "        while True:\n",
    "            data = await queue.get()\n",
    "            # 줄바꿈 인식 위해 json타입으로\n",
    "            json_data = json.dumps({\"data\": data})\n",
    "            yield f\"data: {json_data}\\n\\n\"\n",
    "\n",
    "    return StreamingResponse(\n",
    "        event_generator(),\n",
    "        headers=headers,\n",
    "        media_type=\"text/event-stream\",\n",
    "    )\n",
    "\n",
    "###### 평가 ######\n",
    "\n",
    "@app.post('/eval')\n",
    "async def edit_orion(input_text: InputText):\n",
    "    # 입력값 변수\n",
    "    item = input_text.item\n",
    "    item_text = input_text.text\n",
    "\n",
    "    print('평가 아이템 확인', item)\n",
    "\n",
    "    # 프롬프트\n",
    "    prompt = f\"\"\"너는 자기소개서 첨삭 전문가야.\n",
    "주어진 자기소개서를 분석해서 평가해야해.\n",
    "참고할 사항은 다음과 같아\n",
    "1. '저는', '제가', '저의'같은 1인칭 주어시첨 표현은 제한적으로 사용.\n",
    "2. '귀사', '당사', '이 회사' 같은 표현보단 기업의 정식명칭 사용.\n",
    "3. 문장 구조나 띄어쓰기에 대한 평가는 하지 않음.\n",
    "\n",
    "출력은 다음의 형식을 따라\n",
    "[평가]\n",
    "이런 점이 좋아요:\n",
    "n. (소제목):\n",
    "n. (소제목):\n",
    "...\n",
    "다시 생각해 봐요:\n",
    "n. (소제목):\n",
    "n. (소제목):\n",
    "...\n",
    "\n",
    "다음이 자기소개서야 :\n",
    "[{item_text}]\"\"\"\n",
    "    formatted_prompt = f\"instruction: {prompt}\\n output:\"\n",
    "    example_input = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"stream\": False, # assume the non-streaming case\n",
    "    \"request_id\": item_text,\n",
    "    }\n",
    "\n",
    "    # 비동기 추론\n",
    "    results_generator = engine.generate(\n",
    "        example_input[\"prompt\"],\n",
    "        sampling_params,\n",
    "        example_input[\"request_id\"])\n",
    "\n",
    "    final_output = None\n",
    "    async for request_output in results_generator:\n",
    "        # front단 실시간 추론을 위한 코드\n",
    "        await queue.put(request_output.outputs[0].text)\n",
    "        final_output = request_output\n",
    "        \n",
    "    return final_output.outputs[0].text\n",
    "\n",
    "\n",
    "###### 첨삭 ######\n",
    "\n",
    "@app.post('/editing')\n",
    "async def edit_orion(input_text: InputText):\n",
    "    # 입력값 변수\n",
    "    item = input_text.item\n",
    "    item_text = input_text.text\n",
    "\n",
    "    print('첨삭 아이템 확인', item)\n",
    "    \n",
    "    # 프롬프트\n",
    "    prompt = f\"\"\"너는 자기소개서 첨삭 전문가야.\n",
    "주어진 자기소개서를 첨삭해서 다시 작성해야해.\n",
    "출력형식은 다음을 지켜야해.\n",
    "\n",
    "[첨삭]\n",
    "\n",
    "다음이 자기소개서야 :\n",
    "[{item_text}]\"\"\"\n",
    "    formatted_prompt = f\"instruction: {prompt}\\n output:\"\n",
    "    example_input = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"stream\": False, # assume the non-streaming case\n",
    "    \"request_id\": item_text,\n",
    "    }\n",
    "\n",
    "    # 비동기 추론\n",
    "    results_generator = engine.generate(\n",
    "        example_input[\"prompt\"],\n",
    "        sampling_params,\n",
    "        example_input[\"request_id\"])\n",
    "\n",
    "    final_output = None\n",
    "    async for request_output in results_generator:\n",
    "        # front단 실시간 추론을 위한 코드\n",
    "        await queue.put(request_output.outputs[0].text)\n",
    "        final_output = request_output\n",
    "        \n",
    "    return final_output.outputs[0].text\n",
    "\n",
    "\n",
    "###### 인재상 첨삭 ######\n",
    "@app.post('/talent')\n",
    "async def edit_orion(talent_text: TalentText):\n",
    "        # 입력값 변수\n",
    "    talent = talent_text.talent\n",
    "    user_answer = talent_text.text\n",
    "    req_id = talent_text.req_id\n",
    "    \n",
    "\n",
    "    # 프롬프트\n",
    "    prompt = f\"\"\"\n",
    "당신은 주어진 인재상을 기준으로 사용자의 자기소개서를 효과적으로 수정하는 역할을 수행합니다. 이 과정을 따라 작업을 진행하세요.\n",
    "\n",
    "[입력 정보]\n",
    "인재상: {talent}\n",
    "자기소개서: {user_answer}\n",
    "\n",
    "[첨삭 과정]\n",
    "1. 자기소개서의 각 문단을 신중하게 검토하여 주요 업적과 경험을 식별하고, 이를 인재상의 각 특성과 어떻게 연결할 수 있는지 평가하세요.\n",
    "2. 인재상을 자세히 읽고, 각 특성이 요구하는 주요 요소들을 이해한 후, 지원자의 경험과 이를 연결하세요.\n",
    "3. 각 인재상에 부합하는 경험과 성과를 강조할 부분을 식별하고, 인재상과 직접적인 연관이 없는 부분에는 연결 문장을 추가하여 자연스럽게 연계되도록 하세요.\n",
    "4. 지원자의 능력과 인재상이 어떻게 일치하는지 명확히 보여줄 수 있는 구체적인 예시나 사례를 추가하세요.\n",
    "5. 자기소개서의 전체적인 흐름을 유지하면서, 자연스러운 문장 구성과 일관된 문체로 인재상을 강조하도록 내용을 통합하세요.\n",
    "6. 수정된 자기소개서를 처음부터 끝까지 읽어보며 자연스러움과 인재상의 반영 여부를 확인하고, 모든 인재상이 충분히 반영되었는지 검토하세요.\n",
    "\n",
    "[첨삭]\n",
    ":\n",
    "\n",
    "\"\"\"\n",
    "    formatted_prompt = f\"instruction: {prompt}\\n output:\"\n",
    "\n",
    "    example_input = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"stream\": False, # assume the non-streaming case\n",
    "    \"request_id\": req_id,\n",
    "    }\n",
    "\n",
    "    # 비동기 추론\n",
    "    results_generator = engine.generate(\n",
    "        example_input[\"prompt\"],\n",
    "        sampling_params_tl,\n",
    "        example_input[\"request_id\"]\n",
    "    )\n",
    "\n",
    "    final_output = None\n",
    "    async for request_output in results_generator:\n",
    "        # front단 실시간 추론을 위한 코드\n",
    "        await queue.put(request_output.outputs[0].text)\n",
    "        final_output = request_output\n",
    "        \n",
    "    return final_output.outputs[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236d053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cefb5f1e-d2f2-4340-a098-11c83ce5617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [2683843]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on https://0.0.0.0:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     152.32.251.44:33938 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     152.32.251.44:42254 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     152.32.251.44:42264 - \"GET /robots.txt HTTP/1.1\" 404 Not Found\n",
      "INFO:     152.32.251.44:42274 - \"GET /sitemap.xml HTTP/1.1\" 404 Not Found\n",
      "INFO:     87.236.176.224:55489 - \"GET / HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Event loop stopped before Future completed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43muvicorn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/etc/letsencrypt/live/model.cpm-ai.site/privkey.pem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/etc/letsencrypt/live/model.cpm-ai.site/fullchain.pem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/uvicorn/main.py:577\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    575\u001b[0m         Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m         \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39muds \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config\u001b[38;5;241m.\u001b[39muds):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/uvicorn/server.py:65\u001b[0m, in \u001b[0;36mServer.run\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nest_asyncio.py:96\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mresult()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Event loop stopped before Future completed."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8001, ssl_keyfile=\"/etc/letsencrypt/live/model.cpm-ai.site/privkey.pem\", ssl_certfile=\"/etc/letsencrypt/live/model.cpm-ai.site/fullchain.pem\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3257d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce106e-8e50-46d8-8a95-8f2cc9e28745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5058a18d-87d4-4bc1-a1db-2ca66533bc90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f574008-05e5-48f6-bf81-394fb3509c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b24a3a-a37a-47bd-84e8-988cd3fa480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레거시 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec6c0e-31fc-4af7-9183-59b6f9694dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\", \"https://www.cpm-ai.site\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class InputText(BaseModel):\n",
    "    text: str\n",
    "    item: str\n",
    "\n",
    "class TalentText(BaseModel):\n",
    "    req_id: str\n",
    "    talent: str\n",
    "    text: str\n",
    "\n",
    "# 프론트단 실시간 추론\n",
    "queue = asyncio.Queue()\n",
    "\n",
    "@app.get(\"/sse_01\", response_class=StreamingResponse)\n",
    "async def sse(request: Request):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"text/event-stream;charset=utf-8\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "    async def event_generator():\n",
    "        while True:\n",
    "            data = await queue.get()\n",
    "            # 줄바꿈 인식 위해 json타입으로\n",
    "            json_data = json.dumps({\"data\": data})\n",
    "            yield f\"data: {json_data}\\n\\n\"\n",
    "\n",
    "    return StreamingResponse(\n",
    "        event_generator(),\n",
    "        headers=headers,\n",
    "        media_type=\"text/event-stream\",\n",
    "    )\n",
    "\n",
    "###### 평가 ######\n",
    "\n",
    "@app.post('/eval')\n",
    "async def edit_orion(input_text: InputText):\n",
    "    # 입력값 변수\n",
    "    item = input_text.item\n",
    "    item_text = input_text.text\n",
    "\n",
    "    print('평가 아이템 확인', item)\n",
    "\n",
    "    # 프롬프트\n",
    "    prompt = f\"\"\"너는 자기소개서 첨삭 전문가야.\n",
    "주어진 자기소개서를 분석해서 평가해야해.\n",
    "참고할 사항은 다음과 같아\n",
    "1. '저는', '제가', '저의'같은 1인칭 주어시첨 표현은 제한적으로 사용.\n",
    "2. '귀사', '당사', '이 회사' 같은 표현보단 기업의 정식명칭 사용.\n",
    "3. 문장 구조나 띄어쓰기에 대한 평가는 하지 않음.\n",
    "\n",
    "출력은 다음의 형식을 따라\n",
    "[평가]\n",
    "이런 점이 좋아요:\n",
    "n. (소제목):\n",
    "n. (소제목):\n",
    "...\n",
    "다시 생각해 봐요:\n",
    "n. (소제목):\n",
    "n. (소제목):\n",
    "...\n",
    "\n",
    "다음이 자기소개서야 :\n",
    "[{item_text}]\"\"\"\n",
    "    formatted_prompt = f\"instruction: {prompt}\\n output:\"\n",
    "    example_input = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"stream\": False, # assume the non-streaming case\n",
    "    \"request_id\": item_text,\n",
    "    }\n",
    "\n",
    "    # 비동기 추론\n",
    "    results_generator = engine.generate(\n",
    "        example_input[\"prompt\"],\n",
    "        sampling_params,\n",
    "        example_input[\"request_id\"])\n",
    "\n",
    "    final_output = None\n",
    "    async for request_output in results_generator:\n",
    "        # front단 실시간 추론을 위한 코드\n",
    "        await queue.put(request_output.outputs[0].text)\n",
    "        final_output = request_output\n",
    "        \n",
    "    return final_output.outputs[0].text\n",
    "\n",
    "\n",
    "###### 첨삭 ######\n",
    "\n",
    "@app.post('/editing')\n",
    "async def edit_orion(input_text: InputText):\n",
    "    # 입력값 변수\n",
    "    item = input_text.item\n",
    "    item_text = input_text.text\n",
    "\n",
    "    print('첨삭 아이템 확인', item)\n",
    "    \n",
    "    # 프롬프트\n",
    "    prompt = f\"\"\"너는 자기소개서 첨삭 전문가야.\n",
    "주어진 자기소개서를 첨삭해서 다시 작성해야해.\n",
    "출력형식은 다음을 지켜야해.\n",
    "\n",
    "[첨삭]\n",
    "\n",
    "다음이 자기소개서야 :\n",
    "[{item_text}]\"\"\"\n",
    "    formatted_prompt = f\"instruction: {prompt}\\n output:\"\n",
    "    example_input = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"stream\": False, # assume the non-streaming case\n",
    "    \"request_id\": item_text,\n",
    "    }\n",
    "\n",
    "    # 비동기 추론\n",
    "    results_generator = engine.generate(\n",
    "        example_input[\"prompt\"],\n",
    "        sampling_params,\n",
    "        example_input[\"request_id\"])\n",
    "\n",
    "    final_output = None\n",
    "    async for request_output in results_generator:\n",
    "        # front단 실시간 추론을 위한 코드\n",
    "        await queue.put(request_output.outputs[0].text)\n",
    "        final_output = request_output\n",
    "        \n",
    "    return final_output.outputs[0].text\n",
    "\n",
    "\n",
    "###### 인재상 첨삭 ######\n",
    "@app.post('/talent')\n",
    "async def edit_orion(talent_text: TalentText):\n",
    "        # 입력값 변수\n",
    "    talent = talent_text.talent\n",
    "    user_answer = talent_text.text\n",
    "    req_id = talent_text.req_id\n",
    "    \n",
    "\n",
    "    # 프롬프트\n",
    "    prompt = f\"\"\"당신은 인재상을 기준으로 사용자의 자기소개서를 수정하는 역할을 맡고 있습니다. 아래의 첨삭과정을 지키면서 작업을 수행하세요.\n",
    "\n",
    "[입력 정보]\n",
    "인재상: {talent}\n",
    "자기소개서: {user_answer}\n",
    "\n",
    "[첨삭 과정]\n",
    "1. 인재상을 확인하세요.\n",
    "2. 제공된 자기소개서의 내용과 흐름을 분석하세요.\n",
    "3. 자기소개서에서 지원자의 인성 및 역량 중 인재상과 부합하는 부분을 식별하세요.\n",
    "4. 식별된 부분을 강조하고, 해당 문단에서 인재상과 관련된 경험이나 역량을 구체적으로 설명하세요.\n",
    "5. 모든 문단에 걸쳐 인재상이 고르게 반영되도록 하세요. \n",
    "6. 결론은 제외하세요.\n",
    "7. 출력형식은 다음을 지키세요.\n",
    "\n",
    "[첨삭]\n",
    "\"\"\"\n",
    "    formatted_prompt = f\"instruction: {prompt}\\n output:\"\n",
    "\n",
    "    example_input = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"stream\": False, # assume the non-streaming case\n",
    "    \"request_id\": req_id,\n",
    "    }\n",
    "\n",
    "    # 비동기 추론\n",
    "    results_generator = engine.generate(\n",
    "        example_input[\"prompt\"],\n",
    "        sampling_params,\n",
    "        example_input[\"request_id\"]\n",
    "    )\n",
    "\n",
    "    final_output = None\n",
    "    async for request_output in results_generator:\n",
    "        # front단 실시간 추론을 위한 코드\n",
    "        await queue.put(request_output.outputs[0].text)\n",
    "        final_output = request_output\n",
    "        \n",
    "    return final_output.outputs[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb19a6-5a81-425c-8a0d-cec9eec23793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb0508-2565-42a2-8e74-1e753bac66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### 인재상 첨삭 ######\n",
    "@app.post('/talent')\n",
    "async def edit_orion(talent_text: TalentText):\n",
    "        # 입력값 변수\n",
    "    talent = talent_text.talent\n",
    "    user_answer = talent_text.text\n",
    "    req_id = talent_text.req_id\n",
    "    \n",
    "\n",
    "    # 프롬프트\n",
    "    prompt = f\"\"\"'{talent}'라는 기업의 인재상을 참고하여 자기소개서를 수정해줘\n",
    "\n",
    "다음이 자기소개서야 :\n",
    "[{user_answer}]\"\"\"\n",
    "    formatted_prompt = f\"instruction: {prompt}\\n output:\"\n",
    "\n",
    "    example_input = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"stream\": False, # assume the non-streaming case\n",
    "    \"request_id\": req_id,\n",
    "    }\n",
    "\n",
    "    # 비동기 추론\n",
    "    results_generator = engine.generate(\n",
    "        example_input[\"prompt\"],\n",
    "        sampling_params,\n",
    "        example_input[\"request_id\"]\n",
    "    )\n",
    "\n",
    "    final_output = None\n",
    "    async for request_output in results_generator:\n",
    "        # front단 실시간 추론을 위한 코드\n",
    "        await queue.put(request_output.outputs[0].text)\n",
    "        final_output = request_output\n",
    "        \n",
    "    return final_output.outputs[0].text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soo_vllm",
   "language": "python",
   "name": "soo_vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
